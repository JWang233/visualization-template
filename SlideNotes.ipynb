{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Logistic Regression as a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notations:**\n",
    "\n",
    "- Dataset:  $(x,y), x\\in \\mathbb{R}^{n_{x}}, y \\in \\{0, 1\\}$, where $n_{x}$ is the number of pixels in all bands\n",
    "\n",
    "- Training and Testing data: $ m_{train/test} = \\{(x^{1}, y^{1}), ..., (x^{m}, y^{m}) \\} $\n",
    "\n",
    "- Input data for neural network: \n",
    "    - $$X = [x^{1}, x^{2}, ..., x^{m}] $$\n",
    "    where $X \\in \\mathbb{R}^{n_{x} \\times m}$\n",
    "    \n",
    "    - $$Y = [y^{1}, y^{2}, ..., y^{m}] $$\n",
    "    where $X \\in \\mathbb{R}^{1 \\times m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Concept:**\n",
    "\n",
    "\n",
    "- Given $X$ wants $\\hat{y} = P(y=1 | x)$\n",
    "\n",
    "    where $X \\in \\mathbb{R}^{n_{x}}$, $0 \\leq \\hat{y} \\leq 1$\n",
    "- Parameters: $\\underline{\\omega} \\in \\mathbb{R}^{n_{x}}$, $b \\in \\mathbb{R}$\n",
    "\n",
    "\n",
    "- Outputs: \n",
    "\n",
    "    $ \\hat{y} = \\sigma \\: ( \\omega ^{T}x + b) $\n",
    "\n",
    "    where $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
    "    \n",
    "    <img src = "//github.com/JWang233/visualization-template/blob/master/imgs/sigmoid.png">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss (Error) Function:**\n",
    "\n",
    "$$\\mathcal{L} (\\hat{y}, y) = -( y \\;\\log \\hat{y} + ( 1 - y) \\; log( 1 - \\hat{y}) ) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost Function:**\n",
    "\n",
    "$$J(w,b) = \\frac{1}{m} \\; \\sum _{i=1}^{m} \\mathcal{L} (\\hat{y} \\; ^{(i)}, y^{(i)}) = - \\frac{1}{m} \\; \\sum _{i=1}^{m}[ y^{(i)} \\log \\hat{y} \\; ^{(i)} + ( 1 - y^{(i)} )  log( 1 - \\hat{y} \\; ^{(i)})) ]  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Difference between Loss & Cost Function:**\n",
    "- The loss function computes the error for a single training example\n",
    "- The cost function is the average of the loss functions of the entire training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn\\Train the Parameters**\n",
    "\n",
    "- Goal: want to find $w,b$ that minimize $J(w,b)$, which is defined as a convex function for simplfying the optimization\n",
    "\n",
    "<img src = 'imgs\\gd-ill.jpg'>\n",
    "\n",
    "- Algorithm:\n",
    "\n",
    "    Repeat:\n",
    "$$w \\: := \\: w - \\alpha \\; \\frac{\\partial J(w,b)}{\\partial w} $$\n",
    "$$b \\: := \\: b - \\alpha \\; \\frac{\\partial J(w,b)}{\\partial b} $$\n",
    "    \n",
    "    where $\\alpha$ is the learning rate controlling how big a step we take on each iteration, $\\frac{\\partial J(w)}{\\partial w}$, $dw$ used in the following section, is the partial derivative on $w$ representing the basic update or the change we want to make to the parameter $w$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Derivatives Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives with a Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent on m Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
